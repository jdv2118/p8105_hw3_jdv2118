---
title: "p8105_hw3_jdv2118"
author: "Justin Vargas"
output: github_document
---

# Loading libraries and settings for plots

```{r, message = FALSE}
library(tidyverse)

library(janitor)

library(knitr)

library(ggridges)

library(patchwork)

opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(
  theme_minimal() + theme(legend.position = "bottom")
  )

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d

scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

The code below for problem 1 was provided by Dr. Goldsmith prior to the submission of HW 3 and will be used for future reference.

#### Read in the data

```{r}
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

#### Answer questions about the data

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns, with each row resprenting a single product from an instacart order. Variables include identifiers for user, order, and product; the order in which each product was added to the cart. There are several order-level variables, describing the day and time of the order, and number of days since prior order. Then there are several item-specific variables, describing the product name (e.g. Yogurt, Avocado), department (e.g. dairy and eggs, produce), and aisle (e.g. yogurt, fresh fruits), and whether the item has been ordered by this user in the past. In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users.

Below is a table summarizing the number of items ordered from aisle. In total, there are 134 aisles, with fresh vegetables and fresh fruits holding the most items ordered by far.

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Next is a plot that shows the number of items ordered in each aisle. Here, aisles are ordered by ascending number of items.

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

```

Our next table shows the three most popular items in aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`, and includes the number of times each item is ordered in your table.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

Finally is a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. This table has been formatted in an untidy manner for human readers. Pink Lady Apples are generally purchased slightly earlier in the day than Coffee Ice Cream, with the exception of day 5.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```

# Problem 2

## Data cleaning and tidying

```{r, message = FALSE}
accelerometer_data = 
  read_csv("data/accel_data.csv") %>%
  clean_names() %>%
  pivot_longer(
  activity_1:activity_1440,
    names_to = "minute",
    values_to = "activity_count",
    names_prefix = "activity_") %>%
  mutate(
      day = factor(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")),
    weekday_vs_weekend =
    if_else(day %in% c("Saturday", "Sunday"), 'Weekend', 'Weekday'), 
    minute = as.numeric(minute))
    
accelerometer_data
```

The aforementioned dataset, which the data for has been tidied and wrangled, focuses on accelerometer data that was collected from a 63 year-old male, who has a BMI of 25 and has been diagnosed with congestive heart failure.  The dataset consists of `r ncol(accelerometer_data)` columns and `r nrow(accelerometer_data)` rows.  The 6 variables within the dataset are "week", "day_id", "day", "minute", "activity_count", and "weekday_vs_weekend".  There are 4 variables in the dataset that have a double class and they are "week", "day_id", "minute", and "activity_count".  The "day" variable has a factor data type and "weekday_vs_weekend" is a character data type.  It is important to note that the "weekday_vs_weekend" variable was not part of the original dataset and was created once the data was tidied and wrangled to differentiate whether the name of the day in the "day" variable was either a weekday or a weekend.

## Total activity count table

```{r}
total_activity_table =  
  accelerometer_data %>%
  group_by(week, day) %>%
  summarize(
    total_activity = sum(activity_count, na.rm = TRUE) #Is the na.rm = TRUE necessary?
  ) %>%
  pivot_wider(
  names_from = "day",
  values_from = "total_activity"
) %>%
clean_names()

total_activity_table
```

The code above is used to create a table that consists of the total activity counts for each day that data was collected.  This table does illustrate apparent trends.  One trend is that for all weekdays, the total activity counts increase from the first week to the fifth week for each corresponding weekday.  Another trend is that the total activity counts on Sunday decrease from the first week to the fifth week.  One other trend is that the total activity counts on Saturday decrease from the first week to the fifth week.  Another trend is that the total activity counts increase from Monday to Sunday for the first and second week.  However, the total activity counts decrease from Monday to Sunday for the third, fourth, and fifth weeks.

## Scatterplot of activity counts by minute of each day

```{r}
ggplot(accelerometer_data, aes(x = minute, y = activity_count, color = day)) + 
  geom_point(alpha = .5) +
  labs(
    x = "Minutes of a Day",
    y = "Activity Counts",
    title = "Scatterplot of Activity Counts Per Minute of Each Day"
  )
```

The code above is used to create a scatterplot of activity counts by minutes of the day.  One pattern that can be observed from this scatterplot is that the activity counts are lowest at the beginning of each day of the week.  Another pattern is that some activity counts for certain days are the highest roughly between the 1200 to 1300 minutes of the day, which would be roughly between 8:00pm and 9:40pm.  Also, the activity counts generally increase after the 250 minute mark for each day and stay relatively stable until the 1300 minute mark.  After the 1300 minute mark, the activity counts for each day decrease, but are still higher than they were at the beginning of each day.

# Problem 3 

## Loading data 

```{r}
library(p8105.datasets)

data("ny_noaa")

ny_noaa
```

The code above is used to load the "p8105.datasets" package and load the dataset called "ny_noaa".  The dataset consists of `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns.  There are 7 variables, which are "id", "date", "prcp", "snow", "snwd", "tmax", and "tmin".  All of the variables except for "id" and "date" have a integer data type, while "date" has a date data type and "id" has a character data type.  It is important to note that missing data can be found in all variables except for "date" and "id".  Missing data may cause issues when computing summaries of the data and may affect the plots that will be created using the data.

## Data cleaning and tidying 
```{r}
ny_noaa_tidy =
  ny_noaa %>%
separate(col = date,   into = c("year", "month", "day"), sep = "-") %>%
mutate(
   year = as.numeric(year),
   month = as.numeric(month),
   day = as.numeric(day),
   tmax = as.numeric(tmax),
   tmin = as.numeric(tmin),
   tmax = tmax / 10,
   tmin = tmin / 10,
   prcp = prcp / 10
)

ny_noaa_tidy
```

The code above was used to separate variables for "year", "month", and "day".  The variables of "tmax", "tmin", "prcp" were changed to ensure that they were in reasonable units.  The variable, "snowfall", was not changed as it was already in a reasonable unit.
```{r}
ny_noaa_snow = 
  ny_noaa_tidy %>%
  count(snow) %>%
  arrange(desc(n))

ny_noaa_snow
```

The code above is used to create a table of the most commonly observed values of snowfall, which are "0", "NA", and "25", respectively in that order.  "0" and "NA" may be the most commonly observed values for snowfall as it either does not snow at a weather station or data is not collected in regards to snowfall at a weather station.   

## Line plot of average maximum temperature by year for January and July

```{r}
  ny_noaa_tidy %>% 
  group_by(id, year, month) %>%
  filter(
    month %in% c("1", "7")
  ) %>% 
  mutate(month = recode(month, '1' = "January", '7' = "July")) %>%
  summarize(
    average_tmax = mean(tmax, na.rm = TRUE)
  ) %>%
  ggplot(aes(x = year, y = average_tmax, group = id)) +
  geom_line(alpha = 0.5) +
  labs(
    x = "Year",
    y = "Average Maximum Temperature (C)",
    title = "Average Maximum Temperature vs. year by weather station in January and July",
  ) +
  facet_grid(. ~ month)
```

The code above is used to create a two-panel plot of average maximum temperature by year for the months of January and July.  Based on the computed two-panel plot, we can see that the average maximum temperature in July is higher than the average maximum temperature in January across all years from 1980 to 2010.

```{r}
outliers =
ny_noaa_tidy %>% 
  group_by(id, year, month) %>%
  filter(
    month %in% c("1", "7")
  ) %>% 
  summarize(
    average_tmax = mean(tmax, na.rm = TRUE)
  )

summary(pull(outliers, average_tmax))
```

Based on the computed two-panel plot, there are outliers.  For example, one outlier is `r min(pull(outliers, average_tmax)`, which is the minimum value in the dataset for the average maximum temperature, and another outlier is `r max(pull(outliers, average_tmax)`, which is the maximum value in the dataset for the average maximum temperature.

```{r}
tmax_and_tmin =
ny_noaa_tidy %>% 
ggplot(aes(x = tmin, y = tmax)) +
  geom_hex() +
    labs(
    x = "Minimum Temperature (C)",
    y = "Maximum Temperature (C)",
    title = "Maximum Vs Minimum Temperatures"
    ) + 
  theme(legend.position = "bottom", text = element_text(size = 7))


snowfall_distribution =
  ny_noaa_tidy %>% 
  filter(snow > 0 & snow < 100) %>%
  ggplot(aes(x = year, y = factor(snow))) +
  geom_density_ridges() +
  labs(
    x = "Year",
    y = "Snowfall (mm)",
    title = "Snowfall Values by Year"
    ) 

tmax_and_tmin / snowfall_distribution
```

The code above is used to create a two-panel plot that shows a plot of tmax by tmin for the whole dataset and another plot shows the distribution of snowfall values that are greater than 0 and less than 100 separately by year.  The first plot shows that the most common maximum temperature is roughly 20 degrees celsius and the most common minimum temperature is 15 degrees celsius.  The most common minimum temperatures can roughly be found between -15 degrees celsius and 15 degrees celsius, while the most common maximum temperatures can be roughly found between 0 and 30 degrees celsius.

The second plot shows that the snowfall values are the highest around 2010.  The plot also shows that snowfall values are also high between 1980 and 1990.
